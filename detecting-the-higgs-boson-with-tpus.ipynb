{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Detecting Higgs Boson Particle with TPUs**"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[],"source":["# Model Configuration\n","UNITS = 2 ** 11 # 2048\n","ACTIVATION = 'relu'\n","DROPOUT = 0.1\n","\n","# Training Configuration\n","BATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best"]},{"cell_type":"markdown","metadata":{},"source":["The next few sections set up the TPU computation, data pipeline, and neural network model. If you'd just like to see the results, feel free to skip to the end!\n","\n","# Setup #\n","\n","In addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them."]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-07-13 18:52:57.493832: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"]},{"name":"stdout","output_type":"stream","text":["Tensorflow version 2.5.3\n","Number of accelerators:  1\n"]}],"source":["# TensorFlow\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","# Detect and init the TPU\n","try: # detect TPUs\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","except ValueError: # detect GPUs\n","    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n","print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n","    \n","# Plotting\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Matplotlib defaults\n","plt.style.use('seaborn-whitegrid')\n","plt.rc('figure', autolayout=True)\n","plt.rc('axes', labelweight='bold', labelsize='large',\n","       titleweight='bold', titlesize=18, titlepad=10)\n","\n","\n","# Data\n","# from kaggle_datasets import KaggleDatasets\n","# from tensorflow.io import FixedLenFeature\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","\n","# Model\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import callbacks"]},{"cell_type":"markdown","metadata":{},"source":["Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n","\n","# Load Data #\n","\n","The dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training."]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["def make_decoder(feature_description):\n","    def decoder(example):\n","        example = tf.io.parse_single_example(example, feature_description)\n","        features = tf.io.parse_tensor(example['features'], tf.float32)\n","        features = tf.reshape(features, [28])\n","        label = example['label']\n","        return features, label\n","    return decoder\n","\n","def load_dataset(filenames, decoder, ordered=False):\n","    AUTO = tf.data.experimental.AUTOTUNE\n","    ignore_order = tf.data.Options()\n","    if not ordered:\n","        ignore_order.experimental_deterministic = False\n","    dataset = (\n","        tf.data\n","        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n","        .with_options(ignore_order)\n","        .map(decoder, AUTO)\n","    )\n","    return dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["dataset_size = int(11e6)\n","validation_size = int(5e5)\n","training_size = dataset_size - validation_size\n","\n","# For model.fit\n","batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n","steps_per_epoch = training_size // batch_size\n","validation_steps = validation_size // batch_size\n","\n","# For model.compile\n","steps_per_execution = 256"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-07-13 18:53:00.607934: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n","2022-07-13 18:53:00.625863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:00.626329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n","pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5\n","coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\n","2022-07-13 18:53:00.626363: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","2022-07-13 18:53:00.649504: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n","2022-07-13 18:53:00.649609: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n","2022-07-13 18:53:00.687389: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n","2022-07-13 18:53:00.692053: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n","2022-07-13 18:53:00.696724: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n","2022-07-13 18:53:00.716128: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n","2022-07-13 18:53:00.717226: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n","2022-07-13 18:53:00.717422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:00.717721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:00.718176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n","2022-07-13 18:53:00.719801: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-07-13 18:53:00.720584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:00.720761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n","pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design computeCapability: 7.5\n","coreClock: 1.335GHz coreCount: 24 deviceMemorySize: 5.80GiB deviceMemoryBandwidth: 268.26GiB/s\n","2022-07-13 18:53:00.720899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:00.721055: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:00.721161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n","2022-07-13 18:53:00.721536: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","2022-07-13 18:53:01.546895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2022-07-13 18:53:01.546924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n","2022-07-13 18:53:01.546933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n","2022-07-13 18:53:01.547120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:01.547324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:01.547491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-07-13 18:53:01.547626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4665 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\n"]}],"source":["feature_description = {\n","    'features': tf.io.FixedLenFeature([], tf.string),\n","    'label': tf.io.FixedLenFeature([], tf.float32),\n","}\n","decoder = make_decoder(feature_description)\n","\n","data_dir = '../higgs_boson_detection/datasets/'\n","train_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\n","valid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n","\n","ds_train = load_dataset(train_files, decoder, ordered=False)\n","ds_train = (\n","    ds_train\n","    .cache()\n","    .repeat()\n","    .shuffle(2 ** 19)\n","    .batch(batch_size)\n","    .prefetch(AUTO)\n",")\n","\n","ds_valid = load_dataset(valid_files, decoder, ordered=False)\n","ds_valid = (\n","    ds_valid\n","    .batch(batch_size)\n","    .cache()\n","    .prefetch(AUTO)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Model #\n","\n","Now that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The argument `steps_per_execution` is no longer experimental. Pass `steps_per_execution` instead of `experimental_steps_per_execution`.\n"]}],"source":["def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n","    def make(inputs):\n","        x = layers.Dense(units)(inputs)\n","        x = layers.BatchNormalization()(x)\n","        x = layers.Activation(activation)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","        return x\n","    return make\n","\n","with strategy.scope():\n","    # Wide Network\n","    wide = keras.experimental.LinearModel()\n","\n","    # Deep Network\n","    inputs = keras.Input(shape=[28])\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n","    outputs = layers.Dense(1)(x)\n","    deep = keras.Model(inputs=inputs, outputs=outputs)\n","    \n","    # Wide and Deep Network\n","    wide_and_deep = keras.experimental.WideDeepModel(\n","        linear_model=wide,\n","        dnn_model=deep,\n","        activation='sigmoid',\n","    )\n","\n","wide_and_deep.compile(\n","    loss='binary_crossentropy',\n","    optimizer='adam',\n","    metrics=['AUC', 'binary_accuracy'],\n","    experimental_steps_per_execution=steps_per_execution,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Training #\n","\n","During training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch."]},{"cell_type":"code","execution_count":7,"metadata":{"lines_to_next_cell":2,"trusted":true},"outputs":[],"source":["early_stopping = callbacks.EarlyStopping(\n","    patience=2,\n","    min_delta=0.001,\n","    restore_best_weights=True,\n",")\n","\n","lr_schedule = callbacks.ReduceLROnPlateau(\n","    patience=0,\n","    factor=0.2,\n","    min_lr=0.001,\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n"]},{"name":"stderr","output_type":"stream","text":["2022-07-13 18:53:04.160571: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n","2022-07-13 18:53:04.198779: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2601325000 Hz\n","2022-07-13 18:53:12.396446: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n","2022-07-13 18:53:13.440959: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n"]},{"name":"stdout","output_type":"stream","text":["5126/5126 [==============================] - 942s 184ms/step - loss: 0.5062 - auc: 0.8282 - binary_accuracy: 0.7466 - val_loss: 0.4745 - val_auc: 0.8517 - val_binary_accuracy: 0.7676\n","Epoch 2/50\n","4608/5126 [=========================>....] - ETA: 42s - loss: 0.4717 - auc: 0.8534 - binary_accuracy: 0.7691 "]}],"source":["history = wide_and_deep.fit(\n","    ds_train,\n","    validation_data=ds_valid,\n","    epochs=50,\n","    steps_per_epoch=steps_per_epoch,\n","    validation_steps=validation_steps,\n","    callbacks=[early_stopping, lr_schedule],\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history_frame = pd.DataFrame(history.history)\n","history_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\n","history_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.12 ('rapids-22.02')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"vscode":{"interpreter":{"hash":"86468c90cac0f8646058a0a909ee741a583b9966e79e5a9e5d3dcdec03b52931"}}},"nbformat":4,"nbformat_minor":4}
